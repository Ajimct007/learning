{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ddq4admG0AXU"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch import nn, save, load\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Data\n",
        "train = datasets.MNIST(root='data', download=True, train=True, transform=ToTensor())\n",
        "dataset = DataLoader(train, 32)"
      ],
      "metadata": {
        "id": "kcRsNAi88tIH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Classifier Neural Network\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, (3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, (3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, (3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*(28-6)*(28-6), 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "k8YAVTrt8tFF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instance of the Neural Network, loss, optimizer\n",
        "clf = ImageClassifier().to('cuda')\n",
        "opt = Adam(clf.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "QT86b11r8tBv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training flow\n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(10): # train for 10 epochs\n",
        "        for batch in dataset:\n",
        "            X,y = batch\n",
        "            X, y = X.to('cuda'), y.to('cuda')\n",
        "            yhat = clf(X)\n",
        "            loss = loss_fn(yhat, y)\n",
        "\n",
        "            # Apply backprop\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        print(f\"Epoch:{epoch} loss is {loss.item()}\")\n",
        "\n",
        "    with open('model_state.pt', 'wb') as f:\n",
        "        save(clf.state_dict(), f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG68nmLd8syl",
        "outputId": "f4aa8e92-0772-4710-a91d-7199990f3259"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0 loss is 0.009437127970159054\n",
            "Epoch:1 loss is 0.00428627198562026\n",
            "Epoch:2 loss is 0.03292979300022125\n",
            "Epoch:3 loss is 0.00015004379383753985\n",
            "Epoch:4 loss is 4.1183928260579705e-05\n",
            "Epoch:5 loss is 2.8653952540480532e-05\n",
            "Epoch:6 loss is 4.838952918362338e-06\n",
            "Epoch:7 loss is 2.042753476416692e-05\n",
            "Epoch:8 loss is 6.5226627157244366e-06\n",
            "Epoch:9 loss is 1.713632116207009e-07\n"
          ]
        }
      ]
    }
  ]
}